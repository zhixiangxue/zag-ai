# Retriever Fusion ç­–ç•¥è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç» `QueryFusionRetriever` æ”¯æŒçš„ä¸‰ç§èåˆç­–ç•¥ï¼šSIMPLEã€RECIPROCAL_RANK å’Œ RELATIVE_SCOREã€‚

---

## æ¦‚è¿°

å½“ä½¿ç”¨å¤šä¸ª Retriever è¿›è¡Œæ£€ç´¢æ—¶ï¼Œéœ€è¦å°†å®ƒä»¬çš„ç»“æœèåˆï¼ˆmerge/fusionï¼‰æˆä¸€ä¸ªç»Ÿä¸€çš„ç»“æœåˆ—è¡¨ã€‚ä¸åŒçš„èåˆç­–ç•¥æœ‰ä¸åŒçš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚

---

## 1. SIMPLE - ç®€å•èåˆ

### åŸç†

- å¯¹æ¥è‡ªå¤šä¸ª retriever çš„ç»“æœè¿›è¡Œå»é‡
- å¯¹äºé‡å¤çš„ unitï¼Œä¿ç•™æœ€é«˜çš„åˆ†æ•°
- ç›´æ¥æŒ‰åˆ†æ•°æ’åºè¿”å›

### ç®—æ³•æµç¨‹

```python
for each retriever:
    results = retriever.retrieve(query)
    for each unit in results:
        if unit_id in merged_results:
            # ä¿ç•™æœ€é«˜åˆ†
            merged_results[unit_id].score = max(existing_score, new_score)
        else:
            merged_results[unit_id] = unit

# æŒ‰åˆ†æ•°é™åºæ’åº
return sorted(merged_results, key=lambda x: x.score, reverse=True)
```

### ä¼˜ç‚¹

- âœ… **å®ç°ç®€å•**ï¼šé€»è¾‘æœ€ç›´è§‚ï¼Œæ˜“äºç†è§£å’Œè°ƒè¯•
- âœ… **è®¡ç®—å¼€é”€å°**ï¼šåªéœ€è¦ç®€å•çš„å»é‡å’Œæ’åºæ“ä½œ
- âœ… **ä¿ç•™åŸå§‹åˆ†æ•°**ï¼šä¸æ”¹å˜åˆ†æ•°çš„åŸå§‹è¯­ä¹‰ï¼Œä¾¿äºè¿½æº¯

### ç¼ºç‚¹

- âŒ **åˆ†æ•°ä¸å¯æ¯”**ï¼šä¸åŒ retriever çš„åˆ†æ•°å¯èƒ½åœ¨ä¸åŒçš„å°ºåº¦ä¸Š
  - ç¤ºä¾‹ï¼šRetriever A çš„åˆ†æ•°èŒƒå›´æ˜¯ 0.8-0.95ï¼ŒRetriever B çš„åˆ†æ•°èŒƒå›´æ˜¯ 0.3-0.6
  - ç»“æœï¼šRetriever A çš„ç»“æœä¼šå®Œå…¨å‹å€’ Retriever B
- âŒ **æ’åä¿¡æ¯ä¸¢å¤±**ï¼šåªçœ‹åˆ†æ•°ï¼Œå¿½ç•¥äº†æ’åä½ç½®çš„é‡è¦æ€§
- âŒ **å®¹æ˜“è¢«ä¸»å¯¼**ï¼šåˆ†æ•°æ™®éåé«˜çš„ retriever ä¼šå‹å€’å…¶ä»–çš„

### é€‚ç”¨åœºæ™¯

- ğŸ¯ **ç›¸åŒç±»å‹çš„ retriever**ï¼šå¤šä¸ªä½¿ç”¨ç›¸åŒ embedder çš„ VectorRetriever
- ğŸ¯ **å¿«é€ŸåŸå‹éªŒè¯**ï¼šéœ€è¦å¿«é€Ÿç»„åˆç»“æœè¿›è¡Œæµ‹è¯•
- ğŸ¯ **åˆ†æ•°å·²å½’ä¸€åŒ–**ï¼šç¡®ä¿¡æ‰€æœ‰ retriever çš„åˆ†æ•°åœ¨åŒä¸€å°ºåº¦ä¸Š

### ä½¿ç”¨ç¤ºä¾‹

```python
from zag.retrievers import QueryFusionRetriever, FusionMode

# å¤šä¸ªç›¸åŒç±»å‹çš„ retriever
fusion = QueryFusionRetriever(
    retrievers=[retriever1, retriever2],
    mode=FusionMode.SIMPLE,
    top_k=10
)

results = fusion.retrieve("What is machine learning?")
```

---

## 2. RECIPROCAL_RANK - å€’æ•°æ’åèåˆ (RRF)

### åŸç†

ä½¿ç”¨ RRF (Reciprocal Rank Fusion) ç®—æ³•ï¼ŒåŸºäºæ’åè€Œéåˆ†æ•°è¿›è¡Œèåˆã€‚

**å…¬å¼**ï¼š
```
score(unit) = Î£ [1 / (k + rank_i)]
```

å…¶ä¸­ï¼š
- `k` æ˜¯å¸¸æ•°ï¼ˆé»˜è®¤ 60ï¼Œæ¥è‡ªä¿¡æ¯æ£€ç´¢é¢†åŸŸçš„ç»éªŒå€¼ï¼‰
- `rank_i` æ˜¯è¯¥ unit åœ¨ç¬¬ i ä¸ª retriever ä¸­çš„æ’åï¼ˆä» 0 å¼€å§‹ï¼‰
- å¯¹æ‰€æœ‰ retriever çš„å€’æ•°æ’åæ±‚å’Œ

### ç®—æ³•ç¤ºä¾‹

å‡è®¾ä¸€ä¸ª unit åœ¨ä¸‰ä¸ª retriever ä¸­çš„æ’ååˆ†åˆ«æ˜¯ï¼šç¬¬ 1ã€ç¬¬ 3ã€æœªå‡ºç°

```python
# Retriever 1: rank = 0 (ç¬¬1å)
contribution_1 = 1 / (60 + 0) = 1/60 = 0.0167

# Retriever 2: rank = 2 (ç¬¬3åï¼Œç´¢å¼•ä»0å¼€å§‹)
contribution_2 = 1 / (60 + 2) = 1/62 = 0.0161

# Retriever 3: æœªå‡ºç°ï¼Œä¸è´¡çŒ®åˆ†æ•°

# æœ€ç»ˆåˆ†æ•°
final_score = 0.0167 + 0.0161 = 0.0328
```

### ç‰¹æ€§åˆ†æ

**æ’åè¶Šé å‰ï¼Œè´¡çŒ®è¶Šå¤§**ï¼š
- ç¬¬ 1 åï¼š1/60 â‰ˆ 0.0167
- ç¬¬ 2 åï¼š1/61 â‰ˆ 0.0164
- ç¬¬ 3 åï¼š1/62 â‰ˆ 0.0161
- ç¬¬ 10 åï¼š1/69 â‰ˆ 0.0145

**å¤šä¸ª retriever éƒ½è¿”å›çš„ unit ä¼šå¾—åˆ°æ›´é«˜åˆ†æ•°**ï¼ˆæ°‘ä¸»æŠ•ç¥¨æœºåˆ¶ï¼‰

### ä¼˜ç‚¹

- âœ… **ä¸ä¾èµ–åŸå§‹åˆ†æ•°**ï¼šåªä½¿ç”¨æ’åï¼Œå®Œå…¨é¿å…äº†ä¸åŒå°ºåº¦çš„é—®é¢˜
- âœ… **å¹³è¡¡å„ä¸ª retriever**ï¼šæ¯ä¸ª retriever çš„è´¡çŒ®ç›¸å¯¹å‡è¡¡
- âœ… **å¯¹æ’åé å‰çš„æ›´æ•æ„Ÿ**ï¼šè‡ªåŠ¨ç»™äºˆé«˜æ’åç»“æœæ›´å¤šæƒé‡
- âœ… **ç†è®ºåŸºç¡€æ‰å®**ï¼šåœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸè¢«å¹¿æ³›éªŒè¯å’Œä½¿ç”¨
- âœ… **æ°‘ä¸»æŠ•ç¥¨æœºåˆ¶**ï¼šè¢«å¤šä¸ª retriever è®¤å¯çš„ç»“æœä¼šè·å¾—æ›´é«˜åˆ†æ•°

### ç¼ºç‚¹

- âŒ **å¿½ç•¥åŸå§‹åˆ†æ•°**ï¼šå¦‚æœåŸå§‹ç›¸ä¼¼åº¦åˆ†æ•°å¾ˆé‡è¦ï¼ŒRRF ä¼šä¸¢å¤±è¿™äº›ä¿¡æ¯
  - ä¾‹å¦‚ï¼šç¬¬1ååˆ†æ•° 0.95 å’Œç¬¬1ååˆ†æ•° 0.65 åœ¨ RRF ä¸­è´¡çŒ®ç›¸åŒ
- âŒ **éœ€è¦å®Œæ•´æ’åº**ï¼šå¿…é¡»å¯¹æ¯ä¸ª retriever çš„ç»“æœè¿›è¡Œæ’åº
- âŒ **å‚æ•° k éœ€è¦è°ƒæ•´**ï¼šk=60 æ˜¯ç»éªŒå€¼ï¼Œä¸åŒåœºæ™¯å¯èƒ½éœ€è¦è°ƒä¼˜

### é€‚ç”¨åœºæ™¯

- ğŸ¯ **ä¸åŒç±»å‹çš„ retriever**ï¼šå‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢ï¼ˆæœ€ç»å…¸åœºæ™¯ï¼‰
- ğŸ¯ **åˆ†æ•°ä¸å¯æ¯”**ï¼šå¤šä¸ª retriever çš„åˆ†æ•°å°ºåº¦å®Œå…¨ä¸åŒ
- ğŸ¯ **æ’åæ¯”åˆ†æ•°æ›´é‡è¦**ï¼šæ›´å…³å¿ƒ"å“ªäº›ç»“æœæ’åœ¨å‰é¢"
- ğŸ¯ **æ··åˆæ£€ç´¢ (Hybrid Search)**ï¼šä¸šç•Œæ ‡å‡†åšæ³•

### ä½¿ç”¨ç¤ºä¾‹

```python
from zag.retrievers import VectorRetriever, KeywordRetriever, QueryFusionRetriever, FusionMode

# å‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢
vector_retriever = VectorRetriever(vector_store=chroma_store)
keyword_retriever = KeywordRetriever(keyword_store=meilisearch_store)

# ä½¿ç”¨ RRF èåˆï¼ˆç»å…¸ hybrid searchï¼‰
fusion = QueryFusionRetriever(
    retrievers=[vector_retriever, keyword_retriever],
    mode=FusionMode.RECIPROCAL_RANK,
    top_k=10
)

results = fusion.retrieve("semantic search algorithms")
```

---

## 3. RELATIVE_SCORE - ç›¸å¯¹åˆ†æ•°èåˆ

### åŸç†

é€šè¿‡ MinMax å½’ä¸€åŒ–å°†ä¸åŒ retriever çš„åˆ†æ•°ç»Ÿä¸€åˆ°ç›¸åŒå°ºåº¦ï¼Œç„¶ååº”ç”¨æƒé‡è¿›è¡ŒåŠ æƒèåˆã€‚

### ç®—æ³•æµç¨‹

**æ­¥éª¤ 1ï¼šMinMax å½’ä¸€åŒ–**
```python
for each retriever:
    scores = [unit.score for unit in results]
    min_score = min(scores)
    max_score = max(scores)
    
    for unit in results:
        normalized_score = (unit.score - min_score) / (max_score - min_score)
```

**æ­¥éª¤ 2ï¼šåº”ç”¨æƒé‡**
```python
weighted_score = normalized_score Ã— retriever_weight
```

**æ­¥éª¤ 3ï¼šç´¯åŠ åˆ†æ•°**
```python
for each unit:
    if unit appears in multiple retrievers:
        final_score = sum(weighted_scores from all retrievers)
```

### å½’ä¸€åŒ–ç¤ºä¾‹

å‡è®¾ Retriever A çš„åˆ†æ•°èŒƒå›´æ˜¯ [0.3, 0.8]ï¼ŒæŸä¸ª unit å¾—åˆ† 0.65ï¼š

```python
normalized = (0.65 - 0.3) / (0.8 - 0.3) = 0.35 / 0.5 = 0.7
```

å¦‚æœæƒé‡æ˜¯ 0.6ï¼Œåˆ™åŠ æƒåˆ†æ•°ä¸ºï¼š`0.7 Ã— 0.6 = 0.42`

### ä¼˜ç‚¹

- âœ… **ä¿ç•™åˆ†æ•°ä¿¡æ¯**ï¼šå½’ä¸€åŒ–åä»ä¿ç•™åŸå§‹åˆ†æ•°çš„ç›¸å¯¹å…³ç³»
- âœ… **æ”¯æŒæƒé‡è°ƒèŠ‚**ï¼šå¯ä»¥æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´ä¸åŒ retriever çš„é‡è¦æ€§
- âœ… **åˆ†æ•°å¯è§£é‡Š**ï¼šæœ€ç»ˆåˆ†æ•°æ˜¯å½’ä¸€åŒ–åçš„åŠ æƒå’Œï¼Œå«ä¹‰æ¸…æ™°
- âœ… **é€‚åˆç›¸åŒç±»å‹**ï¼šå¤šä¸ªåŒç±» retriever ä½†æ¥æºä¸åŒæ—¶æ•ˆæœå¥½
- âœ… **ç²¾ç»†æ§åˆ¶**ï¼šå¯ä»¥é€šè¿‡æƒé‡ç²¾ç¡®æ§åˆ¶æ¯ä¸ªæ•°æ®æºçš„å½±å“åŠ›

### ç¼ºç‚¹

- âŒ **å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ**ï¼šMinMax å½’ä¸€åŒ–å®¹æ˜“å—æå€¼å½±å“
  - ç¤ºä¾‹ï¼šå¦‚æœæŸä¸ª unit å¾—åˆ†ç‰¹åˆ«é«˜ï¼ˆå¼‚å¸¸å€¼ï¼‰ï¼Œä¼šå‹ç¼©å…¶ä»–æ‰€æœ‰åˆ†æ•°
- âŒ **éœ€è¦åˆç†æƒé‡**ï¼šæƒé‡è®¾ç½®ä¸å½“ä¼šå¯¼è‡´æŸäº› retriever è¢«è¾¹ç¼˜åŒ–
- âŒ **è®¡ç®—å¼€é”€ç¨å¤§**ï¼šéœ€è¦å¯¹æ¯ä¸ª retriever çš„ç»“æœè®¡ç®— min/max

### é€‚ç”¨åœºæ™¯

- ğŸ¯ **å¤šä¸ªç›¸åŒç±»å‹çš„ retriever**ï¼šå¤šä¸ª VectorRetrieverï¼ˆä¸åŒå‘é‡åº“ï¼‰
- ğŸ¯ **æœ‰æ˜ç¡®çš„æƒé‡åå¥½**ï¼šæŸä¸ªæ•°æ®æºæ›´å¯é ï¼Œæƒ³ç»™æ›´é«˜æƒé‡
- ğŸ¯ **åˆ†æ•°æœ‰å®é™…æ„ä¹‰**ï¼šç›¸ä¼¼åº¦åˆ†æ•°çš„ç»å¯¹å€¼å¾ˆé‡è¦
- ğŸ¯ **è”é‚¦æ£€ç´¢**ï¼šä»å¤šä¸ªç‹¬ç«‹çš„å‘é‡åº“ä¸­æ£€ç´¢å¹¶èåˆ

### ä½¿ç”¨ç¤ºä¾‹

```python
from zag.retrievers import VectorRetriever, QueryFusionRetriever, FusionMode

# ä¸¤ä¸ªä¸åŒçš„å‘é‡æ•°æ®åº“
chroma_retriever = VectorRetriever(vector_store=chroma_store)
pinecone_retriever = VectorRetriever(vector_store=pinecone_store)

# ä½¿ç”¨ç›¸å¯¹åˆ†æ•°èåˆï¼Œç»™ Chroma æ›´é«˜æƒé‡ï¼ˆæ›´å¯é ï¼‰
fusion = QueryFusionRetriever(
    retrievers=[chroma_retriever, pinecone_retriever],
    mode=FusionMode.RELATIVE_SCORE,
    top_k=10,
    retriever_weights=[0.7, 0.3]  # Chroma: 70%, Pinecone: 30%
)

results = fusion.retrieve("vector database comparison")
```

---

## å¯¹æ¯”æ€»ç»“

### ç‰¹æ€§å¯¹æ¯”è¡¨

| ç»´åº¦ | SIMPLE | RECIPROCAL_RANK | RELATIVE_SCORE |
|------|--------|-----------------|----------------|
| **è®¡ç®—å¤æ‚åº¦** | ä½ | ä¸­ | ä¸­é«˜ |
| **æ˜¯å¦éœ€è¦å½’ä¸€åŒ–** | å¦ | å¦ï¼ˆä»…ç”¨æ’åï¼‰ | æ˜¯ |
| **æ˜¯å¦æ”¯æŒæƒé‡** | å¦ | å¦ | æ˜¯ |
| **å¯¹åˆ†æ•°å°ºåº¦æ•æ„Ÿåº¦** | é«˜ | ä½ | ä½ï¼ˆå½’ä¸€åŒ–åï¼‰ |
| **æ˜¯å¦ä¿ç•™åŸå§‹åˆ†æ•°è¯­ä¹‰** | æ˜¯ | å¦ | éƒ¨åˆ†ä¿ç•™ |
| **é€‚åˆå¼‚æ„æ£€ç´¢** | âŒ | âœ… | âŒ |
| **é€‚åˆåŒæ„æ£€ç´¢** | âœ… | âš ï¸ | âœ… |
| **å¯è§£é‡Šæ€§** | é«˜ | ä¸­ | é«˜ |

### æ€§èƒ½å¯¹æ¯”

| ç­–ç•¥ | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ | å¤‡æ³¨ |
|------|-----------|-----------|------|
| SIMPLE | O(n) | O(n) | n ä¸ºæ€»ç»“æœæ•° |
| RECIPROCAL_RANK | O(n log n) | O(n) | éœ€è¦æ’åº |
| RELATIVE_SCORE | O(n) | O(n) | éœ€è¦ä¸¤æ¬¡éå†ï¼ˆmin/max + å½’ä¸€åŒ–ï¼‰|

---

## é€‰æ‹©å»ºè®®

### å†³ç­–æ ‘

```
æ˜¯å¦æ˜¯ä¸åŒç±»å‹çš„ retriever (å¦‚å‘é‡+å…³é”®è¯)?
â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ RECIPROCAL_RANK
â””â”€ å¦ â†’ æ˜¯å¦éœ€è¦ç²¾ç»†æ§åˆ¶æƒé‡?
    â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ RELATIVE_SCORE
    â””â”€ å¦ â†’ åˆ†æ•°æ˜¯å¦åœ¨åŒä¸€å°ºåº¦?
        â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ SIMPLE (æœ€å¿«)
        â””â”€ å¦ â†’ ä½¿ç”¨ RELATIVE_SCORE
```

### åœºæ™¯æ¨è

#### 1. å¤šä¸ªå‘é‡æ•°æ®åº“ï¼ˆç›¸åŒ embedderï¼‰

**æ¨èï¼šRELATIVE_SCORE**

```python
fusion = QueryFusionRetriever(
    retrievers=[chroma_retriever, pinecone_retriever],
    mode=FusionMode.RELATIVE_SCORE,
    retriever_weights=[0.6, 0.4]  # Chroma æ›´å¯é 
)
```

**åŸå› **ï¼š
- åŒç±»å‹ retrieverï¼Œä½†åˆ†æ•°å°ºåº¦å¯èƒ½ä¸åŒ
- å¯ä»¥æ ¹æ®æ•°æ®æºè´¨é‡è®¾ç½®æƒé‡
- ä¿ç•™åˆ†æ•°çš„ç›¸å¯¹å…³ç³»

#### 2. å‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢ï¼ˆæ··åˆæ£€ç´¢ï¼‰

**æ¨èï¼šRECIPROCAL_RANK**

```python
fusion = QueryFusionRetriever(
    retrievers=[vector_retriever, keyword_retriever],
    mode=FusionMode.RECIPROCAL_RANK
)
```

**åŸå› **ï¼š
- ä¸åŒç±»å‹æ£€ç´¢ï¼Œåˆ†æ•°å®Œå…¨ä¸å¯æ¯”
- RRF æ˜¯ä¸šç•Œå…¬è®¤çš„ hybrid search æ ‡å‡†
- ä¸éœ€è¦äººå·¥è°ƒæ•´æƒé‡

#### 3. å¿«é€ŸåŸå‹ï¼Œç®€å•å»é‡

**æ¨èï¼šSIMPLE**

```python
fusion = QueryFusionRetriever(
    retrievers=[retriever1, retriever2],
    mode=FusionMode.SIMPLE
)
```

**åŸå› **ï¼š
- å®ç°æœ€ç®€å•ï¼Œè°ƒè¯•æ–¹ä¾¿
- æ€§èƒ½æœ€å¥½
- é€‚åˆå¿«é€ŸéªŒè¯æƒ³æ³•

#### 4. å¤šæ•°æ®æºè”é‚¦æ£€ç´¢

**æ¨èï¼šRELATIVE_SCORE**

```python
fusion = QueryFusionRetriever(
    retrievers=[
        internal_kb_retriever,    # å†…éƒ¨çŸ¥è¯†åº“
        public_docs_retriever,    # å…¬å¼€æ–‡æ¡£
        user_docs_retriever,      # ç”¨æˆ·æ–‡æ¡£
    ],
    mode=FusionMode.RELATIVE_SCORE,
    retriever_weights=[0.5, 0.3, 0.2]  # æŒ‰å¯ä¿¡åº¦åˆ†é…æƒé‡
)
```

**åŸå› **ï¼š
- éœ€è¦ç²¾ç»†æ§åˆ¶ä¸åŒæ•°æ®æºçš„å½±å“åŠ›
- å„æ•°æ®æºé‡è¦æ€§ä¸åŒ
- åˆ†æ•°å½’ä¸€åŒ–åå¯æ¯”è¾ƒ

---

## å®ç°ç»†èŠ‚

### SIMPLE å®ç°

```python
def _simple_fusion(self, results: list[list[BaseUnit]]) -> list[BaseUnit]:
    all_units: dict[str, BaseUnit] = {}
    
    for units in results:
        for unit in units:
            unit_id = unit.unit_id
            unit_score = unit.score or 0.0
            
            if unit_id in all_units:
                # ä¿ç•™æœ€é«˜åˆ†
                existing_score = all_units[unit_id].score or 0.0
                if unit_score > existing_score:
                    all_units[unit_id] = unit
            else:
                all_units[unit_id] = unit
    
    return sorted(all_units.values(), key=lambda x: x.score or 0.0, reverse=True)
```

### RECIPROCAL_RANK å®ç°

```python
def _reciprocal_rank_fusion(self, results: list[list[BaseUnit]]) -> list[BaseUnit]:
    k = 60.0  # RRF å¸¸æ•°
    fused_scores: dict[str, float] = {}
    id_to_unit: dict[str, BaseUnit] = {}
    
    for units in results:
        sorted_units = sorted(units, key=lambda x: x.score or 0.0, reverse=True)
        
        for rank, unit in enumerate(sorted_units):
            unit_id = unit.unit_id
            id_to_unit[unit_id] = unit
            
            if unit_id not in fused_scores:
                fused_scores[unit_id] = 0.0
            
            # RRF å…¬å¼
            fused_scores[unit_id] += 1.0 / (k + rank)
    
    # æŒ‰èåˆåˆ†æ•°æ’åº
    sorted_ids = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    
    result_units = []
    for unit_id, score in sorted_ids:
        unit = id_to_unit[unit_id].model_copy()
        unit.score = score
        result_units.append(unit)
    
    return result_units
```

### RELATIVE_SCORE å®ç°

```python
def _relative_score_fusion(self, results: list[list[BaseUnit]]) -> list[BaseUnit]:
    all_units: dict[str, BaseUnit] = {}
    
    for i, units in enumerate(results):
        if not units:
            continue
        
        # æå–åˆ†æ•°å¹¶å½’ä¸€åŒ–
        scores = [unit.score or 0.0 for unit in units]
        min_score = min(scores)
        max_score = max(scores)
        
        for unit in units:
            unit_id = unit.unit_id
            original_score = unit.score or 0.0
            
            # MinMax å½’ä¸€åŒ–
            if max_score == min_score:
                normalized_score = 1.0 if max_score > 0 else 0.0
            else:
                normalized_score = (original_score - min_score) / (max_score - min_score)
            
            # åº”ç”¨æƒé‡
            weighted_score = normalized_score * self.retriever_weights[i]
            
            # ç´¯åŠ åˆ†æ•°
            if unit_id in all_units:
                all_units[unit_id].score += weighted_score
            else:
                unit_copy = unit.model_copy()
                unit_copy.score = weighted_score
                all_units[unit_id] = unit_copy
    
    return sorted(all_units.values(), key=lambda x: x.score or 0.0, reverse=True)
```

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆ RRF çš„ k å€¼æ˜¯ 60ï¼Ÿ

**A**: k=60 æ˜¯ä¿¡æ¯æ£€ç´¢é¢†åŸŸçš„ç»éªŒå€¼ï¼Œæ¥è‡ªå¤šå¹´çš„å®è·µéªŒè¯ã€‚å®ƒçš„ä½œç”¨æ˜¯ï¼š
- å¹³æ»‘æ’åå·®å¼‚ï¼šé¿å…æ’åé å‰çš„ç»“æœåˆ†æ•°è¿‡é«˜
- ä¿æŒåŒºåˆ†åº¦ï¼šåŒæ—¶ä¿è¯æ’åå·®å¼‚ä»èƒ½ä½“ç°åœ¨åˆ†æ•°ä¸Š

ä½ å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ k å€¼ï¼š
- **k è¶Šå°**ï¼šæ’åé å‰çš„ç»“æœæƒé‡è¶Šé«˜ï¼ˆæ›´æ¿€è¿›ï¼‰
- **k è¶Šå¤§**ï¼šæ’åå·®å¼‚å½±å“å˜å°ï¼ˆæ›´ä¿å®ˆï¼‰

### Q2: RELATIVE_SCORE ä¸­æƒé‡å¦‚ä½•è®¾ç½®ï¼Ÿ

**A**: æƒé‡è®¾ç½®å»ºè®®ï¼š
1. **æ ¹æ®æ•°æ®æºè´¨é‡**ï¼šæ›´å¯é çš„æ•°æ®æºç»™æ›´é«˜æƒé‡
2. **æ ¹æ®æ•°æ®æºå¤§å°**ï¼šæ•°æ®é‡å¤§çš„å¯ä»¥ç»™ç¨é«˜æƒé‡
3. **æ ¹æ®ä¸šåŠ¡é‡è¦æ€§**ï¼šæ ¸å¿ƒä¸šåŠ¡æ•°æ®æºç»™æ›´é«˜æƒé‡
4. **A/B æµ‹è¯•è°ƒä¼˜**ï¼šé€šè¿‡å®éªŒæ‰¾åˆ°æœ€ä¼˜æƒé‡ç»„åˆ

ç¤ºä¾‹ï¼š
```python
# ä¸‰ä¸ªæ•°æ®æºï¼šå†…éƒ¨çŸ¥è¯†åº“ã€å…¬å¼€æ–‡æ¡£ã€ç”¨æˆ·ä¸Šä¼ 
weights=[0.5, 0.3, 0.2]  # å†…éƒ¨çŸ¥è¯†åº“æœ€å¯é ï¼Œç”¨æˆ·ä¸Šä¼ æœ€ä¸ç¡®å®š
```

### Q3: å¯ä»¥æ··åˆä½¿ç”¨ä¸åŒç­–ç•¥å—ï¼Ÿ

**A**: å¯ä»¥ï¼`QueryFusionRetriever` æœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ª `BaseRetriever`ï¼Œå¯ä»¥åµŒå¥—ä½¿ç”¨ï¼š

```python
# å…ˆç”¨ RRF èåˆå‘é‡å’Œå…³é”®è¯æ£€ç´¢
hybrid1 = QueryFusionRetriever(
    retrievers=[vector1, keyword1],
    mode=FusionMode.RECIPROCAL_RANK
)

hybrid2 = QueryFusionRetriever(
    retrievers=[vector2, keyword2],
    mode=FusionMode.RECIPROCAL_RANK
)

# å†ç”¨ RELATIVE_SCORE èåˆä¸¤ä¸ªæ··åˆæ£€ç´¢å™¨
final_fusion = QueryFusionRetriever(
    retrievers=[hybrid1, hybrid2],
    mode=FusionMode.RELATIVE_SCORE,
    retriever_weights=[0.6, 0.4]
)
```

### Q4: å¦‚ä½•è¯„ä¼°ä¸åŒç­–ç•¥çš„æ•ˆæœï¼Ÿ

**A**: è¯„ä¼°æ–¹æ³•ï¼š
1. **ç¦»çº¿è¯„ä¼°**ï¼š
   - å‡†å¤‡æµ‹è¯•é›†ï¼ˆquery + ç›¸å…³æ–‡æ¡£æ ‡æ³¨ï¼‰
   - è®¡ç®— MRR (Mean Reciprocal Rank)ã€NDCG ç­‰æŒ‡æ ‡
   - å¯¹æ¯”ä¸åŒç­–ç•¥çš„æŒ‡æ ‡

2. **åœ¨çº¿ A/B æµ‹è¯•**ï¼š
   - çº¿ä¸Šåˆ†æµä¸åŒç­–ç•¥
   - æ”¶é›†ç”¨æˆ·åé¦ˆï¼ˆç‚¹å‡»ç‡ã€æ»¡æ„åº¦ç­‰ï¼‰
   - é€‰æ‹©è¡¨ç°æœ€å¥½çš„ç­–ç•¥

3. **äººå·¥è¯„ä¼°**ï¼š
   - æŠ½æ ·æ£€æŸ¥æ£€ç´¢ç»“æœ
   - è¯„ä¼°ç›¸å…³æ€§å’Œæ’åºè´¨é‡

---

## å‚è€ƒèµ„æ–™

### å­¦æœ¯è®ºæ–‡

1. **RRF ç®—æ³•åŸè®ºæ–‡**:
   - Cormack, G. V., Clarke, C. L., & Buettcher, S. (2009). "Reciprocal rank fusion outperforms condorcet and individual rank learning methods."

2. **æ··åˆæ£€ç´¢ç»¼è¿°**:
   - Zamani, H., et al. (2022). "Retrieval-Enhanced Machine Learning."

### ç›¸å…³é“¾æ¥

- [Elasticsearch RRF æ–‡æ¡£](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html)
- [LlamaIndex Fusion Retriever](https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion/)
- [Vector Search vs Keyword Search](https://www.pinecone.io/learn/hybrid-search-intro/)

---

## æ›´æ–°æ—¥å¿—

- **2026-01-05**: åˆå§‹ç‰ˆæœ¬ï¼Œæ–‡æ¡£åŒ–ä¸‰ç§èåˆç­–ç•¥
