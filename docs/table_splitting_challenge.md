# Table Splitting Challenge: è¡¨æ ¼å¤„ç†çš„å¤šå±‚çº§è®¾è®¡æŒ‘æˆ˜

## é—®é¢˜èƒŒæ™¯

åœ¨åŸºäº Markdown çš„æ–‡æ¡£å¤„ç†ä¸­ï¼ˆç‰¹åˆ«æ˜¯é€šè¿‡ MinerU/Docling è§£æ PDF åç”Ÿæˆçš„ Markdownï¼‰ï¼Œç»å¸¸ä¼šé‡åˆ° **HTML æ ¼å¼çš„è¡¨æ ¼**ï¼ˆå› ä¸º Markdown è¡¨æ ¼ä¸æ”¯æŒ rowspan/colspanï¼‰ã€‚è¿™å¸¦æ¥äº†ä¸€ç³»åˆ—è®¾è®¡æŒ‘æˆ˜ï¼š

1. **Splitter èƒ½å¦æ­£ç¡®åˆ‡åˆ†åŒ…å« HTML è¡¨æ ¼çš„å†…å®¹ï¼Ÿ**
2. **å¦‚ä½•åŒºåˆ† Markdown è¡¨æ ¼å’Œ HTML è¡¨æ ¼ï¼Ÿ**
3. **TableExtractor å¦‚ä½•ç²¾å‡†æ€»ç»“å¤æ‚ HTML è¡¨æ ¼ï¼Ÿ**
4. **å¤æ‚è¡¨æ ¼ä¼šå¼•èµ·å“ªäº›è¿é”ååº”ï¼Ÿ**

---

## æ ¸å¿ƒæŒ‘æˆ˜æ‹†è§£

### 1ï¸âƒ£ MarkdownHeaderSplitter å¯¹ HTML è¡¨æ ¼çš„å¤„ç†èƒ½åŠ›

**å½“å‰å®ç°**: `zag/splitters/markdown/header_based.py`

**èƒ½åŠ›è¯„ä¼°**:
- âœ… æ­£ç¡®å¤„ç† Markdown æ ‡å‡†è¡¨æ ¼ï¼ˆ`| col1 | col2 |`ï¼‰
- âœ… æ­£ç¡®å¤„ç† HTML è¡¨æ ¼å—ï¼ˆä½œä¸ºæ™®é€šæ–‡æœ¬å†…å®¹ä¿ç•™ï¼‰
- âœ… é¿å…åœ¨ä»£ç å—å†…è§£ææ ‡é¢˜
- âŒ **ä¸ä¼šè¯†åˆ«è¡¨æ ¼çš„è¯­ä¹‰è¾¹ç•Œ**ï¼Œåªä¼šæŒ‰æ ‡é¢˜åˆ‡åˆ†

**ç¤ºä¾‹åœºæ™¯**:

```markdown
## äº§å“å¯¹æ¯”

è¿™æ˜¯ä¸€æ®µä»‹ç»æ–‡å­—ã€‚

<table>
  <tr><td>Product</td><td>Price</td></tr>
  <tr><td>A</td><td>100</td></tr>
</table>

è¿™æ˜¯è¡¨æ ¼åçš„æ€»ç»“ã€‚

## ä¸‹ä¸€ç« èŠ‚
...
```

**å½“å‰è¡Œä¸º**: æ•´ä¸ª "äº§å“å¯¹æ¯”" ç« èŠ‚ï¼ˆåŒ…æ‹¬ HTML è¡¨æ ¼ï¼‰ä¼šè¢«åˆ‡åˆ†ä¸º**ä¸€ä¸ª TextUnit**ï¼Œè¡¨æ ¼çš„ HTML ä»£ç ä¼šä¿ç•™åœ¨ `content` ä¸­ã€‚

**é—®é¢˜**: è¡¨æ ¼æ²¡æœ‰è¢«è¯†åˆ«ä¸ºç‹¬ç«‹çš„è¯­ä¹‰å•å…ƒï¼ˆTableUnitï¼‰ï¼Œå½±å“åç»­çš„ï¼š
- å‘é‡æ£€ç´¢æ•ˆæœï¼ˆè¡¨æ ¼å†…å®¹è¢«ç¨€é‡Šåœ¨å¤§æ®µæ–‡æœ¬ä¸­ï¼‰
- å…ƒæ•°æ®æå–ï¼ˆæ— æ³•é’ˆå¯¹è¡¨æ ¼åšç‰¹æ®Šå¤„ç†ï¼‰
- ç»“æ„åŒ–æŸ¥è¯¢ï¼ˆæ— æ³•è¿‡æ»¤è¡¨æ ¼ç±»å‹çš„ Unitï¼‰

---

### 2ï¸âƒ£ å¦‚ä½•åŒºåˆ† Markdown è¡¨æ ¼å’Œ HTML è¡¨æ ¼

#### **æ–¹æ¡ˆ A: åœ¨ Splitter ä¸­å¢å¼ºè¡¨æ ¼è¯†åˆ«**

åˆ›å»ºä¸€ä¸ª `TableAwareSplitter`ï¼Œèƒ½å¤Ÿï¼š

```python
class TableAwareSplitter(BaseSplitter):
    """
    Markdown splitter that recognizes both MD and HTML tables
    
    Logic:
    1. Parse markdown by headers (current logic)
    2. Within each section, detect tables:
       - Markdown tables: | col | col |
       - HTML tables: <table>...</table>
    3. Extract tables as separate TableUnits
    4. Keep text before/after tables as TextUnits
    """
    
    def _detect_tables(self, content: str) -> list[dict]:
        """Detect both MD and HTML tables"""
        tables = []
        
        # Detect HTML tables
        html_pattern = r'<table[^>]*>.*?</table>'
        for match in re.finditer(html_pattern, content, re.DOTALL | re.IGNORECASE):
            tables.append({
                'type': 'html',
                'content': match.group(),
                'start': match.start(),
                'end': match.end()
            })
        
        # Detect Markdown tables (GFM style)
        md_table_pattern = r'(?:^|\n)(\|.+\|(?:\n\|[-:\s|]+\|)(?:\n\|.+\|)*)'
        for match in re.finditer(md_table_pattern, content, re.MULTILINE):
            tables.append({
                'type': 'markdown',
                'content': match.group(1),
                'start': match.start(),
                'end': match.end()
            })
        
        return sorted(tables, key=lambda x: x['start'])
    
    def _do_split(self, document) -> list[BaseUnit]:
        """
        Split markdown with table awareness
        
        1. Detect all tables in content
        2. Split content into segments:
           - Text segments â†’ TextUnit
           - Table segments â†’ TableUnit
        3. Maintain context_path for each unit
        """
        content = document.content
        tables = self._detect_tables(content)
        
        units = []
        last_pos = 0
        
        for table in tables:
            # Add text before table
            if table['start'] > last_pos:
                text_content = content[last_pos:table['start']].strip()
                if text_content:
                    units.append(TextUnit(
                        content=text_content,
                        # ... metadata
                    ))
            
            # Add table unit
            units.append(TableUnit(
                content=table['content'],
                json_data={'table_type': table['type']},
                # ... metadata
            ))
            
            last_pos = table['end']
        
        # Add remaining text
        if last_pos < len(content):
            remaining = content[last_pos:].strip()
            if remaining:
                units.append(TextUnit(content=remaining))
        
        return units
```

**ä¼˜ç‚¹**:
- èƒ½å¤„ç†çº¯ Markdown æ–‡æ¡£ï¼ˆæ²¡æœ‰ç»è¿‡ Reader çš„åœºæ™¯ï¼‰
- ç»Ÿä¸€çš„ Splitter æ¥å£

**ç¼ºç‚¹**:
- æ­£åˆ™è¡¨è¾¾å¼å¯èƒ½ä¸å¤Ÿé²æ£’ï¼ˆåµŒå¥—è¡¨æ ¼ã€è¡¨æ ¼å±æ€§ç­‰ï¼‰
- é‡å¤äº† Reader çš„å·¥ä½œï¼ˆMinerU/Docling å·²ç»è¯†åˆ«è¡¨æ ¼ï¼‰

---

#### **æ–¹æ¡ˆ B: åœ¨ Reader ä¸­ç›´æ¥æ„å»º TableUnitï¼ˆæ¨èï¼‰**

MinerU å’Œ Docling è¿™æ ·çš„ Reader **å·²ç»åšäº†è¡¨æ ¼è¯†åˆ«**ï¼Œå®ƒä»¬çš„è¾“å‡ºä¸­æ˜ç¡®æ ‡æ³¨äº†è¡¨æ ¼ï¼š

**MinerU çš„ content_list è¾“å‡º**:
```python
{
    "type": "table",
    "html": "<table>...</table>",  # â† å·²ç»è¯†åˆ«å‡ºæ¥äº†ï¼
    "latex": "\\begin{tabular}...",
    "page_idx": 2,
    "bbox": [x, y, w, h]
}
```

**Docling çš„è¾“å‡º**:
```python
# DoclingDocument ä¸­çš„ TableItem
table_item = {
    "type": "table",
    "data": {
        "grid": [[cell, cell, ...], ...],
        "num_rows": 5,
        "num_cols": 3
    },
    "prov": [{"bbox": {...}}]
}
```

**æœ€ä¼˜æ–¹æ¡ˆ**: åœ¨ Reader å±‚é¢ï¼ˆå¦‚ `MinerUReader._build_pages_from_content_list`ï¼‰å°±æ„å»º `TableUnit`ï¼š

```python
def _build_pages_from_content_list(self, content_list: list[dict]) -> list[Page]:
    """Build Page objects with TableUnits"""
    
    page_items = {}
    
    for item in content_list:
        page_num = item.get("page_idx", 0) + 1
        
        if page_num not in page_items:
            page_items[page_num] = {
                "units": []  # æ”¹ä¸º units åˆ—è¡¨ï¼Œç»Ÿä¸€å­˜å‚¨
            }
        
        # Classify item type
        item_type = item.get("type", "text")
        
        if item_type == "text":
            # åˆ›å»º TextUnit
            unit = TextUnit(
                unit_id=self.generate_unit_id(),
                content=item.get("text", ""),
                metadata=UnitMetadata(
                    context_path=f"Page{page_num}",
                    custom={
                        "layout_type": item.get("layout_type", "text"),
                        "bbox": item.get("bbox")
                    }
                )
            )
            page_items[page_num]["units"].append(unit)
            
        elif item_type == "table":
            # åˆ›å»º TableUnit
            unit = TableUnit(
                unit_id=self.generate_unit_id(),
                content=item.get("html", ""),  # HTML è¡¨æ ¼
                json_data={
                    "table_type": "html",
                    "raw_html": item.get("html"),
                    "latex": item.get("latex"),  # MinerU è¿˜æä¾› LaTeX
                    "bbox": item.get("bbox"),
                    "page_idx": item.get("page_idx")
                },
                metadata=UnitMetadata(
                    context_path=f"Page{page_num}/Table",
                    custom={"bbox": item.get("bbox")}
                )
            )
            page_items[page_num]["units"].append(unit)
            
        elif item_type in ["image", "figure"]:
            # åˆ›å»º ImageUnit
            unit = ImageUnit(
                unit_id=self.generate_unit_id(),
                content=b"",  # éœ€è¦è¯»å–å›¾ç‰‡äºŒè¿›åˆ¶
                format="png",
                caption=item.get("caption"),
                metadata=UnitMetadata(
                    context_path=f"Page{page_num}/Image",
                    custom={
                        "path": item.get("img_path"),
                        "bbox": item.get("bbox")
                    }
                )
            )
            page_items[page_num]["units"].append(unit)
    
    # Create Page objects
    pages = []
    for page_num in sorted(page_items.keys()):
        units = page_items[page_num]["units"]
        
        # æ„å»º Unit é“¾è¡¨å…³ç³»
        for i in range(len(units)):
            if i > 0:
                units[i].prev_unit_id = units[i - 1].unit_id
            if i < len(units) - 1:
                units[i].next_unit_id = units[i + 1].unit_id
        
        pages.append(Page(
            page_number=page_num,
            content=units,  # ç›´æ¥å­˜å‚¨ Unit åˆ—è¡¨
            metadata={
                "unit_count": len(units)
            }
        ))
    
    return pages
```

**ä¼˜ç‚¹**:
- å¤ç”¨ Reader çš„é«˜ç²¾åº¦è§£æï¼ˆMinerU 82-90+ å‡†ç¡®ç‡ï¼‰
- TableUnit ä¸­åŒ…å«ä¸°å¯Œçš„ç»“æ„åŒ–ä¿¡æ¯ï¼ˆbbox, latex, etc.ï¼‰
- é¿å…é‡å¤è§£æ
- Splitter ä¸éœ€è¦åšè¡¨æ ¼è¯†åˆ«ï¼ˆåªéœ€æŒ‰éœ€åˆ‡åˆ† TextUnitï¼‰

**ç¼ºç‚¹**:
- éœ€è¦ä¿®æ”¹ Reader å®ç°
- Page.content çš„æ•°æ®ç»“æ„éœ€è¦è°ƒæ•´ï¼ˆä» dict æ”¹ä¸º list[BaseUnit]ï¼‰

---

### 3ï¸âƒ£ TableExtractor å¦‚ä½•ç²¾å‡†æ€»ç»“å¤æ‚ HTML è¡¨æ ¼

**å½“å‰å®ç°**: `zag/extractors/table.py`

**å½“å‰ Prompt** (line 58-65):
```python
prompt = f"""ä»¥ä¸‹æ˜¯ä¸€ä¸ªè¡¨æ ¼çš„ç»“æ„åŒ–æ•°æ®ï¼š

{json_data}  # â† è¿™é‡Œæ˜¯ä»€ä¹ˆï¼Ÿ

è¯·ç”¨ 2-3 å¥è¯æ€»ç»“è¿™ä¸ªè¡¨æ ¼çš„å†…å®¹ï¼Œçªå‡ºå…³é”®æ•°æ®å’Œå¯¹æ¯”å…³ç³»ã€‚
"""
```

**é—®é¢˜**:
1. `json_data` çš„æ ¼å¼æ˜¯ä»€ä¹ˆï¼Ÿå¦‚æœæ˜¯ HTML å­—ç¬¦ä¸²ï¼ŒLLM å¯èƒ½ç†è§£ä¸ä½³
2. æ²¡æœ‰è¡¨æ ¼çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€å‰åæ®µè½ï¼‰
3. å¯¹äºå¤æ‚è¡¨æ ¼ï¼ˆå¤šå±‚è¡¨å¤´ã€åˆå¹¶å•å…ƒæ ¼ï¼‰ï¼Œå¯èƒ½ä¸¢å¤±ç»“æ„ä¿¡æ¯

---

#### **æ”¹è¿›æ–¹æ¡ˆ 1: ä½¿ç”¨ Pandas è§£æ HTML è¡¨æ ¼ï¼ˆæ¨èï¼‰**

**é‡å¤§å‘ç°**ï¼šç»è¿‡å®Œæ•´éªŒè¯ï¼Œ**Pandas å¯ä»¥å®Œç¾å¤„ç†æ‰€æœ‰å¤æ‚ HTML è¡¨æ ¼**ï¼

**éªŒè¯è¿‡ç¨‹**ï¼š

æˆ‘ä»¬åˆ›å»ºäº†åŒ…å« 6 ç§å¤æ‚è¡¨æ ¼çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆ`playground/complex_table.html`ï¼‰ï¼š
1. âœ… ç®€å•è¡¨æ ¼ï¼ˆåŸºçº¿ï¼‰
2. âœ… Rowspanï¼ˆåˆå¹¶è¡Œï¼‰- `rowspan="3"`
3. âœ… Colspanï¼ˆåˆå¹¶åˆ—ï¼‰- `colspan="2"`
4. âœ… Rowspan + Colspan æ··åˆ
5. âœ… 3 å±‚è¡¨å¤´ï¼ˆæç«¯å¤æ‚ï¼‰
6. âœ… ç©ºå•å…ƒæ ¼ + æ··åˆå†…å®¹

**æµ‹è¯•ç»“æœ**ï¼šPandas çš„ `pd.read_html()` **6/6 å…¨éƒ¨é€šè¿‡**ï¼

**Pandas çš„å¤„ç†æœºåˆ¶**ï¼š

```python
import pandas as pd
from io import StringIO

# HTML è¡¨æ ¼ï¼ˆå¸¦ rowspan/colspanï¼‰
html = """<table>
  <tr>
    <td rowspan="3">Electronics</td>
    <td>Laptop</td>
    <td>$999</td>
  </tr>
  <tr>
    <td>Phone</td>
    <td>$699</td>
  </tr>
  <tr>
    <td>Tablet</td>
    <td>$499</td>
  </tr>
</table>"""

# Pandas è‡ªåŠ¨å¤„ç† rowspan
df = pd.read_html(StringIO(html))[0]
print(df)
# Output:
#    Category Product Price
# 0  Electronics  Laptop  $999
# 1  Electronics   Phone  $699  â† "Electronics" è‡ªåŠ¨å¤åˆ¶
# 2  Electronics  Tablet  $499  â† "Electronics" è‡ªåŠ¨å¤åˆ¶
```

**å…³é”®ç‰¹æ€§**ï¼š

| ç‰¹æ€§ | Pandas å¤„ç†æ–¹å¼ | éªŒè¯ç»“æœ |
|-----|----------------|----------|
| **Rowspan** | è‡ªåŠ¨å¤åˆ¶å€¼åˆ°åç»­è¡Œ | âœ… å®Œç¾æ”¯æŒ |
| **Colspan** | è½¬ä¸ºå¤šå±‚åˆ—ç´¢å¼•ï¼ˆMultiIndexï¼‰ | âœ… å®Œç¾æ”¯æŒ |
| **å¤šå±‚è¡¨å¤´** | ç”Ÿæˆ MultiIndex columns | âœ… æ”¯æŒ 3 å±‚åµŒå¥— |
| **ç©ºå•å…ƒæ ¼** | å¡«å…… NaNï¼ˆå¯è½¬ç©ºå­—ç¬¦ä¸²ï¼‰ | âœ… å®Œç¾å¤„ç† |
| **è¾“å‡ºæ ¼å¼** | DataFrame â†’ JSON/dict/list | âœ… æ ‡å‡†åŒ– |

**JSON è¾“å‡ºç¤ºä¾‹**ï¼ˆTest 2: Rowspanï¼‰ï¼š

```json
{
  "table_id": "table2",
  "shape": {"rows": 5, "columns": 4},
  "headers": ["Category", "Product", "Price", "Stock"],
  "rows": [
    ["Electronics", "Laptop", "$999", "50"],
    ["Electronics", "Phone", "$699", "120"],
    ["Electronics", "Tablet", "$499", "80"],
    ["Furniture", "Chair", "$199", "30"],
    ["Furniture", "Desk", "$399", "15"]
  ],
  "metadata": {
    "has_multi_level_headers": false,
    "total_cells": 20
  }
}
```

**åŸå§‹ HTML å¯¹æ¯”**ï¼š

```json
{
  "original_html": {
    "total_rows": 6,
    "merged_cells": [
      {
        "cell_text": "Electronics",
        "position": {"row": 1, "col": 0},
        "rowspan": 3,  // â† åŸæœ¬è·¨ 3 è¡Œ
        "colspan": 1
      }
    ]
  },
  "pandas_result": {
    "shape": {"rows": 5, "columns": 4},
    "data_sample": [
      {"Category": "Electronics", "Product": "Laptop", ...},
      {"Category": "Electronics", "Product": "Phone", ...}  // â† è‡ªåŠ¨å¡«å……
    ]
  }
}
```

**ä¸ºä»€ä¹ˆé€‰æ‹© Pandas**ï¼š

1. âœ… **é›¶æ‰‹å·¥å¤„ç†**ï¼šrowspan/colspan è‡ªåŠ¨å±•å¼€
2. âœ… **é²æ£’æ€§å¼º**ï¼šå¤„ç†è¿‡æµ·é‡çœŸå®åœºæ™¯ï¼ˆé‡‘èã€ç§‘ç ”æ•°æ®ï¼‰
3. âœ… **æ ‡å‡†åŒ–è¾“å‡º**ï¼šDataFrame å¯è½»æ¾è½¬ä¸ºä»»ä½•æ ¼å¼
4. âœ… **ç”Ÿæ€æˆç†Ÿ**ï¼šPython æ•°æ®åˆ†æäº‹å®æ ‡å‡†
5. âœ… **ä»£ç ç®€æ´**ï¼š3 è¡Œä»£ç è§£å†³ BeautifulSoup éœ€è¦ 50+ è¡Œçš„é—®é¢˜

---

#### **æ”¹è¿›æ–¹æ¡ˆ 1B: å¢å¼º TableExtractorï¼ˆåŸºäº Pandasï¼‰**

ä¿®æ”¹ Readerï¼Œè®© `TableUnit.json_data` åŒ…å« Pandas è§£æçš„ç»“æ„åŒ–æ•°æ®ï¼š

```python
# åœ¨ MinerUReader ä¸­
{
    "table_type": "html",  # or "markdown"
    "raw_html": "<table>...</table>",
    "parsed_structure": {
        "headers": ["Product", "Q1", "Q2", "Q3"],
        "rows": [
            ["ProductA", "100", "120", "130"],
            ["ProductB", "80", "90", "95"]
        ],
        "merged_cells": [
            {"row": 0, "col": 0, "rowspan": 2, "colspan": 1}
        ]
    },
    "context": {
        "preceding_text": "ä¸‹è¡¨å±•ç¤ºäº†å­£åº¦é”€å”®æ•°æ®ï¼š",
        "following_text": "ä»è¡¨æ ¼å¯ä»¥çœ‹å‡ºï¼ŒProductA å¢é•¿æ›´å¿«ã€‚"
    }
}
```

**å¢å¼ºçš„ TableExtractor**:

```python
import pandas as pd
from io import StringIO

class TableExtractor(BaseExtractor):
    """Enhanced table extractor using Pandas for robust HTML parsing"""
    
    def __init__(self, llm_uri: str, api_key: str):
        self.llm_uri = llm_uri
        self.api_key = api_key
        self._conv = chak.Conversation(llm_uri, api_key=api_key)
    
    async def _extract_from_unit(self, unit) -> Dict:
        if not isinstance(unit, TableUnit):
            return {}
        
        json_data = unit.json_data
        if not json_data:
            return {}
        
        # æ£€æµ‹è¡¨æ ¼ç±»å‹
        table_type = json_data.get("table_type", "unknown")
        
        if table_type == "html":
            # ä½¿ç”¨ Pandas è§£æ HTML è¡¨æ ¼ï¼ˆè‡ªåŠ¨å¤„ç† rowspan/colspanï¼‰
            parsed = self._parse_html_table_with_pandas(json_data["raw_html"])
            
            if not parsed["rows"]:
                return {}
            
            # æ„å»ºç»“æ„åŒ– prompt
            prompt = f"""ä»¥ä¸‹æ˜¯ä¸€ä¸ª HTML è¡¨æ ¼çš„ç»“æ„åŒ–æ•°æ®ï¼š

è¡¨å¤´ï¼š{', '.join(parsed['headers'])}
è¡Œæ•°ï¼š{parsed['shape'][0]}
åˆ—æ•°ï¼š{parsed['shape'][1]}
å¤šå±‚è¡¨å¤´ï¼š{parsed['has_multi_level_headers']}

æ•°æ®æ ·ä¾‹ï¼ˆå‰5è¡Œï¼‰ï¼š
{self._format_rows(parsed['rows'][:5])}

ä¸Šä¸‹æ–‡ï¼š
- å‰æ–‡ï¼š{json_data.get('context', {}).get('preceding_text', 'æ— ')}
- åæ–‡ï¼š{json_data.get('context', {}).get('following_text', 'æ— ')}

è¯·ç”¨ 2-3 å¥è¯æ€»ç»“è¿™ä¸ªè¡¨æ ¼çš„å†…å®¹ï¼Œçªå‡ºå…³é”®æ•°æ®ã€è¶‹åŠ¿å’Œå¯¹æ¯”å…³ç³»ã€‚
è¦æ±‚ï¼šä½¿ç”¨å®Œæ•´çš„å¥å­ï¼Œä¾¿äºå‘é‡æ£€ç´¢ã€‚

æ‘˜è¦ï¼š"""
        else:
            # Markdown æˆ–å…¶ä»–æ ¼å¼
            prompt = f"""ä»¥ä¸‹æ˜¯ä¸€ä¸ªè¡¨æ ¼ï¼š

{unit.content}

è¯·ç”¨ 2-3 å¥è¯æ€»ç»“è¿™ä¸ªè¡¨æ ¼çš„å†…å®¹ã€‚

æ‘˜è¦ï¼š"""
        
        response = await self._conv.asend(prompt)
        return {
            "table_summary": response.content.strip(),
            "table_structure": {
                "row_count": parsed.get("shape", (0, 0))[0],
                "col_count": parsed.get("shape", (0, 0))[1],
                "has_multi_level_headers": parsed.get("has_multi_level_headers", False)
            }
        }
    
    def _parse_html_table_with_pandas(self, html: str) -> dict:
        """
        Parse HTML table using Pandas (handles rowspan/colspan automatically)
        
        Returns:
            {
                "headers": [...],
                "rows": [[...], [...]],
                "shape": (rows, cols),
                "has_multi_level_headers": bool
            }
        """
        try:
            # Pandas è‡ªåŠ¨å¤„ç† rowspan/colspan
            dfs = pd.read_html(StringIO(html))
            
            if not dfs:
                return self._parse_html_table_fallback(html)
            
            df = dfs[0]
            
            # å¤„ç†å¤šå±‚è¡¨å¤´
            if isinstance(df.columns, pd.MultiIndex):
                # å±•å¹³å¤šå±‚è¡¨å¤´ï¼š('Sales', 'Domestic') -> 'Sales | Domestic'
                headers = [' | '.join(map(str, col)).strip() for col in df.columns]
                has_multi_level = True
            else:
                headers = [str(col) for col in df.columns]
                has_multi_level = False
            
            # è½¬ä¸º list of lists
            rows = df.fillna('').astype(str).values.tolist()
            
            return {
                "headers": headers,
                "rows": rows,
                "shape": df.shape,
                "has_multi_level_headers": has_multi_level
            }
        
        except Exception as e:
            # Fallback to BeautifulSoupï¼ˆæå°‘æƒ…å†µï¼‰
            return self._parse_html_table_fallback(html)
    
    def _parse_html_table_fallback(self, html: str) -> dict:
        """Fallback parser using BeautifulSoup"""
        try:
            from bs4 import BeautifulSoup
            
            soup = BeautifulSoup(html, 'html.parser')
            table = soup.find('table')
            
            if not table:
                return {"headers": [], "rows": [], "shape": (0, 0), "has_multi_level_headers": False}
            
            headers = []
            rows = []
            
            # æå–è¡¨å¤´
            thead = table.find('thead')
            if thead:
                header_row = thead.find('tr')
                if header_row:
                    headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]
            
            # æå–æ•°æ®è¡Œ
            tbody = table.find('tbody') or table
            for tr in tbody.find_all('tr'):
                row = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
                if row and row != headers:
                    rows.append(row)
            
            return {
                "headers": headers,
                "rows": rows,
                "shape": (len(rows), len(headers)),
                "has_multi_level_headers": False
            }
        
        except Exception as e:
            return {"headers": [], "rows": [], "shape": (0, 0), "has_multi_level_headers": False}
    
    def _format_rows(self, rows: list) -> str:
        """Format rows for prompt"""
        return "\n".join([f"  {i+1}. {row}" for i, row in enumerate(rows)])
```

**å…³é”®æ”¹è¿›**ï¼š

1. âœ… **ä¸»è§£æå™¨æ”¹ä¸º Pandas**ï¼šè‡ªåŠ¨å¤„ç† rowspan/colspan
2. âœ… **BeautifulSoup ä½œä¸º fallback**ï¼šæå°‘æƒ…å†µæ‰ç”¨åˆ°
3. âœ… **å¤šå±‚è¡¨å¤´æ”¯æŒ**ï¼šè‡ªåŠ¨å±•å¹³ä¸º `"Sales | Domestic"` æ ¼å¼
4. âœ… **è¿”å›æ›´å¤šå…ƒæ•°æ®**ï¼šshape, has_multi_level_headers
5. âœ… **é”™è¯¯å¤„ç†å®Œå–„**ï¼šPandas å¤±è´¥æ—¶ fallback åˆ° BeautifulSoup

---

#### **æ”¹è¿›æ–¹æ¡ˆ 2: ä½¿ç”¨ VLM ç†è§£è¡¨æ ¼**

å¯¹äºéå¸¸å¤æ‚çš„è¡¨æ ¼ï¼ˆå¦‚è´¢æŠ¥ã€ç§‘æŠ€è®ºæ–‡ä¸­çš„å¯¹æ¯”è¡¨ï¼‰ï¼Œå¯ä»¥ï¼š

```python
class VLMTableExtractor(BaseExtractor):
    """Use VLM to understand complex tables"""
    
    def __init__(self, llm_uri: str, api_key: str, use_vlm: bool = False):
        self.llm_uri = llm_uri
        self.api_key = api_key
        self.use_vlm = use_vlm
        
        if use_vlm:
            # åˆå§‹åŒ– VLMï¼ˆå¦‚ GPT-4o, Qwen-VLï¼‰
            self._vlm = chak.Conversation(
                "bailian/qwen-vl-max",  # æ”¯æŒè§†è§‰çš„æ¨¡å‹
                api_key=api_key
            )
        else:
            self._conv = chak.Conversation(llm_uri, api_key=api_key)
    
    async def _extract_from_unit(self, unit: TableUnit) -> Dict:
        if not isinstance(unit, TableUnit):
            return {}
        
        json_data = unit.json_data
        if not json_data or not self.use_vlm:
            # ä½¿ç”¨å¸¸è§„æ–¹æ³•
            return await self._extract_with_llm(unit)
        
        # Option A: Render HTML to image, use VLM
        if json_data.get('table_type') == 'html':
            # Convert HTML table to image
            table_image = self._render_html_to_image(unit.content)
            
            # Use VLM
            response = await self._vlm.asend(
                "è¿™æ˜¯ä¸€ä¸ªè¡¨æ ¼å›¾ç‰‡ï¼Œè¯·æ€»ç»“å…¶å†…å®¹å’Œå…³é”®ä¿¡æ¯ï¼ˆ2-3å¥è¯ï¼‰ã€‚",
                images=[table_image]
            )
            
            return {"table_summary": response.content.strip()}
        
        # Fallback to text-based
        return await self._extract_with_llm(unit)
    
    def _render_html_to_image(self, html: str) -> bytes:
        """Render HTML table to image using selenium or playwright"""
        from playwright.sync_api import sync_playwright
        
        with sync_playwright() as p:
            browser = p.chromium.launch()
            page = browser.new_page()
            
            # Wrap table in full HTML
            full_html = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <style>
                    table {{ border-collapse: collapse; }}
                    td, th {{ border: 1px solid black; padding: 8px; }}
                </style>
            </head>
            <body>{html}</body>
            </html>
            """
            
            page.set_content(full_html)
            screenshot = page.screenshot()
            browser.close()
            
            return screenshot
```

---

#### **æ”¹è¿›æ–¹æ¡ˆ 3: åˆ†å±‚æ€»ç»“ï¼ˆé€‚åˆè¶…å¤§è¡¨æ ¼ï¼‰**

å¯¹äºå‡ åè¡Œçš„å¤æ‚è¡¨æ ¼ï¼š

```python
class HierarchicalTableExtractor(BaseExtractor):
    """Summarize large tables hierarchically"""
    
    def __init__(self, llm_uri: str, api_key: str, chunk_size: int = 5):
        self.llm_uri = llm_uri
        self.api_key = api_key
        self.chunk_size = chunk_size
        self._conv = chak.Conversation(llm_uri, api_key=api_key)
    
    async def _extract_from_unit(self, unit: TableUnit) -> Dict:
        if not isinstance(unit, TableUnit):
            return {}
        
        json_data = unit.json_data
        if not json_data:
            return {}
        
        # è§£æè¡¨æ ¼
        parsed = self._parse_html_table(json_data.get("raw_html", ""))
        rows = parsed["rows"]
        headers = parsed["headers"]
        
        # å¦‚æœè¡¨æ ¼ä¸å¤§ï¼Œç›´æ¥æ€»ç»“
        if len(rows) <= self.chunk_size:
            return await self._summarize_small_table(headers, rows)
        
        # å¤§è¡¨æ ¼ï¼šåˆ†å±‚æ€»ç»“
        # 1. æ¯ N è¡Œç”Ÿæˆä¸€ä¸ªå°æ‘˜è¦
        row_summaries = []
        for i in range(0, len(rows), self.chunk_size):
            chunk = rows[i:i + self.chunk_size]
            summary = await self._summarize_chunk(headers, chunk, i)
            row_summaries.append(summary)
        
        # 2. æ±‡æ€»æ‰€æœ‰å°æ‘˜è¦
        final_summary = await self._summarize_summaries(headers, row_summaries)
        
        return {"table_summary": final_summary}
    
    async def _summarize_chunk(self, headers: list, rows: list, start_idx: int) -> str:
        """Summarize a chunk of rows"""
        prompt = f"""è¡¨å¤´ï¼š{headers}

æ•°æ®è¡Œ {start_idx+1} åˆ° {start_idx+len(rows)}ï¼š
{self._format_rows(rows)}

ç”¨ä¸€å¥è¯æ€»ç»“è¿™éƒ¨åˆ†æ•°æ®çš„ç‰¹ç‚¹ã€‚

æ‘˜è¦ï¼š"""
        
        response = await self._conv.asend(prompt)
        return response.content.strip()
    
    async def _summarize_summaries(self, headers: list, summaries: list) -> str:
        """Summarize all chunk summaries"""
        prompt = f"""è¿™æ˜¯ä¸€ä¸ªå¤§å‹è¡¨æ ¼çš„åˆ†æ®µæ‘˜è¦ï¼š

è¡¨å¤´ï¼š{headers}

åˆ†æ®µæ‘˜è¦ï¼š
{chr(10).join([f"{i+1}. {s}" for i, s in enumerate(summaries)])}

è¯·ç”¨ 2-3 å¥è¯æ€»ç»“æ•´ä¸ªè¡¨æ ¼çš„å†…å®¹ï¼Œçªå‡ºå…³é”®æ•°æ®å’Œè¶‹åŠ¿ã€‚

æœ€ç»ˆæ‘˜è¦ï¼š"""
        
        response = await self._conv.asend(prompt)
        return response.content.strip()
```

---

### 4ï¸âƒ£ è¿é”ååº”ï¼šå¤æ‚è¡¨æ ¼å¦‚ä½•å½±å“æ•´ä¸ª Pipeline

| **é˜¶æ®µ** | **æ½œåœ¨é—®é¢˜** | **è§£å†³æ–¹æ¡ˆ** |
|---------|-------------|-------------|
| **Reader** | HTML è§£æä¸å‡†ç¡®ã€è¡¨æ ¼è¢«è¯†åˆ«ä¸ºæ–‡æœ¬ | ä½¿ç”¨ MinerU/Docling é«˜ç²¾åº¦è§£æå™¨ï¼›åœ¨ Reader å±‚é¢æ„å»º TableUnit |
| **Splitter** | è¡¨æ ¼è¢«åˆ‡æ–­ã€ä¸ä¸Šä¸‹æ–‡åˆ†ç¦»ã€æ— æ³•è¯†åˆ«è¡¨æ ¼è¾¹ç•Œ | ä½¿ç”¨ TableAwareSplitterï¼›æˆ–ä¾èµ– Reader å·²æ„å»ºçš„ TableUnit |
| **Extractor** | LLM æ— æ³•ç†è§£å¤æ‚ HTMLã€ç¼ºå°‘ä¸Šä¸‹æ–‡ã€è¶…é•¿è¡¨æ ¼ | ä½¿ç”¨ BeautifulSoup è§£æ HTMLï¼›æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›ä½¿ç”¨ VLM æˆ–åˆ†å±‚æ€»ç»“ |
| **Embedder** | è¡¨æ ¼å‘é‡åŒ–æ•ˆæœå·®ã€è¯­ä¹‰ä¿¡æ¯ä¸è¶³ | ä¾èµ–é«˜è´¨é‡çš„ `table_summary`ï¼›è€ƒè™‘å•ç‹¬ embedding è¡¨æ ¼å’Œæ‘˜è¦ |
| **Retriever** | æ£€ç´¢ä¸åˆ°è¡¨æ ¼å†…å®¹ã€ç»“æ„åŒ–æŸ¥è¯¢å¤±è´¥ | ç¡®ä¿ `table_summary` è´¨é‡ï¼›æ”¯æŒ metadata è¿‡æ»¤ï¼ˆtable_type, page, bboxï¼‰ |
| **Indexer** | è¡¨æ ¼å’Œæ–‡æœ¬æ··åˆç´¢å¼•æ•ˆæœä¸ä½³ | åˆ†åˆ«ç´¢å¼• TextUnit å’Œ TableUnitï¼›æ”¯æŒæŒ‰ unit_type è¿‡æ»¤ |

---

## æ¨èçš„å®Œæ•´è®¾è®¡æ–¹æ¡ˆ

### **Phase 1: Reader å±‚é¢æ”¹é€ ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰**

**ç›®æ ‡**: è®© Reader ç›´æ¥è¾“å‡º TableUnitï¼Œé¿å…åç»­é‡å¤è§£æ

**æ¶‰åŠæ–‡ä»¶**:
- `zag/readers/mineru.py`
- `zag/readers/docling.py`
- `zag/schemas/pdf.py` (Page çš„æ•°æ®ç»“æ„)

**æ”¹åŠ¨ç‚¹**:

1. **ä¿®æ”¹ Page.content çš„æ•°æ®ç»“æ„**:
   ```python
   # å½“å‰: content æ˜¯ dict
   Page(
       page_number=1,
       content={
           "texts": [...],
           "tables": [...],
           "images": [...]
       }
   )
   
   # æ”¹ä¸º: content æ˜¯ list[BaseUnit]
   Page(
       page_number=1,
       content=[
           TextUnit(...),
           TableUnit(...),
           TextUnit(...),
           ImageUnit(...)
       ]
   )
   ```

2. **åœ¨ `_build_pages_from_content_list` ä¸­æ„å»º TableUnit**ï¼ˆè§ä¸Šæ–‡ä»£ç ï¼‰

3. **TableUnit åŒ…å«ä¸°å¯Œçš„ json_data**:
   ```python
   TableUnit(
       content="<table>...</table>",
       json_data={
           "table_type": "html",
           "raw_html": "...",
           "latex": "...",  # MinerU æä¾›
           "bbox": [x, y, w, h],
           "page_idx": 2
       }
   )
   ```

---

### **Phase 2: TableExtractor å¢å¼ºï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰**

**ç›®æ ‡**: è®© TableExtractor èƒ½å¤Ÿå‡†ç¡®æ€»ç»“å¤æ‚ HTML è¡¨æ ¼

**æ¶‰åŠæ–‡ä»¶**:
- `zag/extractors/table.py`

**æ”¹åŠ¨ç‚¹**:

1. **ä½¿ç”¨ Pandas ä½œä¸ºä¸»è§£æå™¨**ï¼ˆå·²éªŒè¯å¯è¡Œï¼‰
2. æ”¹è¿› Promptï¼ŒåŒ…å«ï¼š
   - ç»“æ„åŒ–çš„è¡¨å¤´å’Œæ•°æ®
   - ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
   - è¡¨æ ¼å¤§å°ï¼ˆè¡Œæ•°ã€åˆ—æ•°ï¼‰
   - å¤šå±‚è¡¨å¤´ä¿¡æ¯
3. å¯é€‰ï¼šæ”¯æŒ VLM æ¨¡å¼
4. å¯é€‰ï¼šæ”¯æŒåˆ†å±‚æ€»ç»“ï¼ˆè¶…å¤§è¡¨æ ¼ï¼‰

**éªŒè¯æ–‡ä»¶**:
- `playground/complex_table.html` - 6 ç§å¤æ‚è¡¨æ ¼æµ‹è¯•ç”¨ä¾‹
- `playground/test_pandas_table_parser.py` - Pandas è§£æéªŒè¯è„šæœ¬
- `playground/pandas_test_output.txt` - å®Œæ•´æµ‹è¯•ç»“æœï¼ˆJSON æ ¼å¼ï¼‰

---

### **Phase 3: TableAwareSplitterï¼ˆä½ä¼˜å…ˆçº§ï¼Œå¯é€‰ï¼‰**

**ç›®æ ‡**: å¤„ç†çº¯ Markdown æ–‡æ¡£ï¼ˆæ²¡æœ‰ç»è¿‡ Reader çš„åœºæ™¯ï¼‰

**ä½¿ç”¨åœºæ™¯**:
- ç”¨æˆ·ç›´æ¥è¯»å– `.md` æ–‡ä»¶
- Markdown ä¸­åŒ…å« HTML è¡¨æ ¼
- ä¸ä½¿ç”¨ MinerU/Docling Reader

**æ¶‰åŠæ–‡ä»¶**:
- `zag/splitters/markdown/table_aware.py` (æ–°å»º)

---

### **Phase 4: æµ‹è¯•å’ŒéªŒè¯**

**æµ‹è¯•ç”¨ä¾‹**:

1. **ç®€å• HTML è¡¨æ ¼**ï¼ˆ3x3ï¼‰
   - éªŒè¯ï¼šèƒ½æ­£ç¡®è¯†åˆ«å’Œæ€»ç»“
   
2. **å¤æ‚ HTML è¡¨æ ¼**ï¼ˆå¤šå±‚è¡¨å¤´ã€åˆå¹¶å•å…ƒæ ¼ï¼‰
   - éªŒè¯ï¼šèƒ½ä¿ç•™ç»“æ„ä¿¡æ¯
   
3. **è¶…å¤§è¡¨æ ¼**ï¼ˆ50+ è¡Œï¼‰
   - éªŒè¯ï¼šåˆ†å±‚æ€»ç»“ä¸ä¼šè¶…è¿‡ token é™åˆ¶
   
4. **æ··åˆå†…å®¹**ï¼ˆæ–‡æœ¬ + è¡¨æ ¼ + å›¾ç‰‡ï¼‰
   - éªŒè¯ï¼šæ‰€æœ‰ Unit çš„é“¾è¡¨å…³ç³»æ­£ç¡®
   
5. **Markdown æ ‡å‡†è¡¨æ ¼ vs HTML è¡¨æ ¼**
   - éªŒè¯ï¼šä¸¤ç§æ ¼å¼éƒ½èƒ½æ­£ç¡®å¤„ç†

---

## å…³é”®è®¾è®¡å†³ç­–

### âœ… **æ¨èåšæ³•**

1. **åœ¨ Reader å±‚é¢è¯†åˆ«è¡¨æ ¼**ï¼šå¤ç”¨ MinerU/Docling çš„é«˜ç²¾åº¦è§£æ
2. **TableUnit ä¿ç•™å¤šç§æ ¼å¼**ï¼š
   - `content`: åŸå§‹ HTML/Markdown
   - `json_data`: ç»“æ„åŒ–æ•°æ® (headers + rows)
   - `metadata.custom`: bbox, page_idx ç­‰
3. **TableExtractor å…ˆè§£æåæ€»ç»“**ï¼šä½¿ç”¨ BeautifulSoup è§£æ HTMLï¼Œå†äº¤ç»™ LLM
4. **å¯¹äºè¶…å¤æ‚è¡¨æ ¼**ï¼šè€ƒè™‘ä½¿ç”¨ VLM æˆ–åˆ†å±‚æ€»ç»“

### âŒ **é¿å…çš„åšæ³•**

1. **ä¸è¦åœ¨ Splitter ä¸­é‡å¤è§£æè¡¨æ ¼**ï¼ˆReader å·²ç»åšäº†ï¼‰
2. **ä¸è¦ç›´æ¥æŠŠ HTML å­—ç¬¦ä¸²å–‚ç»™ LLM**ï¼ˆéœ€è¦å…ˆç»“æ„åŒ–ï¼‰
3. **ä¸è¦å¿½ç•¥è¡¨æ ¼çš„ä¸Šä¸‹æ–‡**ï¼ˆå‰åæ®µè½ã€æ ‡é¢˜ç­‰ï¼‰
4. **ä¸è¦æŠŠè¡¨æ ¼å’Œå¤§æ®µæ–‡æœ¬æ··åœ¨ä¸€ä¸ª TextUnit ä¸­**ï¼ˆå½±å“æ£€ç´¢ï¼‰

---

## å®ç°è·¯çº¿å›¾

| Phase | ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„è®¡å·¥ä½œé‡ | çŠ¶æ€ |
|-------|------|--------|-----------|-----|
| 0 | **éªŒè¯ Pandas å¤„ç†å¤æ‚è¡¨æ ¼èƒ½åŠ›** | ğŸ”´ é«˜ | 2 å°æ—¶ | âœ… **å·²å®Œæˆ** |
| 1 | ä¿®æ”¹ Reader è¾“å‡º TableUnit | ğŸ”´ é«˜ | 2-3 å°æ—¶ | â³ å¾…å®æ–½ |
| 2 | å¢å¼º TableExtractorï¼ˆPandas è§£æï¼‰ | ğŸŸ¡ ä¸­ | 1-2 å°æ—¶ | â³ å¾…å®æ–½ |
| 3 | æ·»åŠ  VLM æ”¯æŒï¼ˆå¯é€‰ï¼‰ | ğŸŸ¢ ä½ | 2-3 å°æ—¶ | â³ å¾…å®æ–½ |
| 4 | å®ç° TableAwareSplitterï¼ˆå¯é€‰ï¼‰ | ğŸŸ¢ ä½ | 2-3 å°æ—¶ | â³ å¾…å®æ–½ |
| 5 | ç¼–å†™æµ‹è¯•ç”¨ä¾‹ | ğŸŸ¡ ä¸­ | 1 å°æ—¶ | â³ å¾…å®æ–½ |

---

## æ¢ç´¢è¿‡ç¨‹ä¸éªŒè¯ç»“æœ

### **éªŒè¯ Pandas å¤„ç†å¤æ‚è¡¨æ ¼çš„èƒ½åŠ›ï¼ˆPhase 0ï¼‰**

**é—®é¢˜**: åœ¨è®¨è®º TableExtractor å¦‚ä½•è§£æ HTML è¡¨æ ¼æ—¶ï¼Œå‘ç°ä¸€ä¸ªæ ¸å¿ƒç–‘é—®ï¼š
> "æ—¢ç„¶å¤æ‚è¡¨æ ¼ LLM ä¸å¥½ç†è§£ï¼Œé‚£ä¹ˆæœ‰æ²¡æœ‰ä¸€äº›ä¼˜ç§€çš„å¼€æºé¡¹ç›®æˆ–è€… lib ä¸“é—¨æŠŠ md æˆ–è€… html çš„è¡¨æ ¼è½¬ä¸º json like çš„æ•°æ®ç»“æ„çš„å‘¢ï¼Ÿbs èƒ½åšåˆ°ä¹ˆï¼Ÿ"

**æ¢ç´¢æ–¹å‘**: è°ƒç ”ä¸»æµè¡¨æ ¼è§£æåº“

#### **å€™é€‰æ–¹æ¡ˆå¯¹æ¯”**

| åº“ | è§£æèƒ½åŠ› | rowspan/colspan | è¾“å‡ºæ ¼å¼ | é€‚ç”¨åœºæ™¯ | æ¨èæŒ‡æ•° |
|---|---------|----------------|---------|---------|----------|
| **Pandas** | â­â­â­â­â­ | âœ… å®Œç¾æ”¯æŒ | DataFrame/JSON/dict | æ‰€æœ‰åœºæ™¯ | â­â­â­â­â­ |
| **BeautifulSoup** | â­â­â­ | âŒ éœ€æ‰‹åŠ¨å¤„ç† | dict/list | ç®€å•è¡¨æ ¼ | â­â­â­ |
| **html-table-parser** | â­â­â­â­ | âœ… å±•å¼€æˆç½‘æ ¼ | list of lists | ä¸­ç­‰å¤æ‚åº¦ | â­â­â­â­ |
| **camelot-py** | â­â­â­â­â­ | âœ… å®Œç¾æ”¯æŒ | DataFrame | ä»… PDF | â­â­â­â­ |

#### **éªŒè¯è¿‡ç¨‹**

**Step 1: åˆ›å»ºæµ‹è¯•ç”¨ä¾‹**

åˆ›å»ºäº† `playground/complex_table.html`ï¼ŒåŒ…å« 6 ç§å¤æ‚åœºæ™¯ï¼š

1. **Test 1**: ç®€å•è¡¨æ ¼ï¼ˆ3x4ï¼ŒåŸºçº¿æµ‹è¯•ï¼‰
2. **Test 2**: Rowspanï¼ˆ`<td rowspan="3">Electronics</td>`ï¼‰
3. **Test 3**: Colspanï¼ˆ`<th colspan="2">Sales</th>`ï¼‰
4. **Test 4**: Rowspan + Colspan æ··åˆï¼ˆ6 ä¸ªåˆå¹¶å•å…ƒæ ¼ï¼‰
5. **Test 5**: 3 å±‚è¡¨å¤´ï¼ˆæç«¯å¤æ‚ï¼‰
6. **Test 6**: ç©ºå•å…ƒæ ¼ + æ··åˆå†…å®¹

**Step 2: ç¼–å†™éªŒè¯è„šæœ¬**

åˆ›å»ºäº† `playground/test_pandas_table_parser.py`ï¼Œæ ¸å¿ƒé€»è¾‘ï¼š

```python
import pandas as pd
from io import StringIO

# Parse HTML table
dfs = pd.read_html(StringIO(html_content))
df = dfs[0]

# Convert to JSON
structured = {
    "table_id": table_id,
    "shape": {"rows": df.shape[0], "columns": df.shape[1]},
    "headers": [str(col) for col in df.columns],
    "rows": df.fillna('').astype(str).values.tolist(),
    "metadata": {
        "has_multi_level_headers": isinstance(df.columns, pd.MultiIndex),
        "total_cells": df.shape[0] * df.shape[1]
    }
}
```

**Step 3: è¿è¡Œæµ‹è¯•**

```bash
python playground/test_pandas_table_parser.py > playground/pandas_test_output.txt
```

**æµ‹è¯•ç»“æœ**: **6/6 å…¨éƒ¨é€šè¿‡** âœ…

#### **å…³é”®å‘ç°**

**1. Rowspan å¤„ç†ï¼ˆTest 2ï¼‰**

åŸå§‹ HTMLï¼š
```html
<td rowspan="3">Electronics</td>  <!-- è·¨ 3 è¡Œ -->
```

Pandas è¾“å‡ºï¼š
```json
{
  "rows": [
    ["Electronics", "Laptop", "$999", "50"],
    ["Electronics", "Phone", "$699", "120"],  // â† è‡ªåŠ¨å¤åˆ¶
    ["Electronics", "Tablet", "$499", "80"]   // â† è‡ªåŠ¨å¤åˆ¶
  ]
}
```

**éªŒè¯**: âœ… Pandas è‡ªåŠ¨å°† "Electronics" å¤åˆ¶åˆ°åç»­ 2 è¡Œï¼

---

**2. Colspan å¤„ç†ï¼ˆTest 3ï¼‰**

åŸå§‹ HTMLï¼š
```html
<th colspan="2">Sales</th>  <!-- è·¨ 2 åˆ— -->
```

Pandas è¾“å‡ºï¼š
```json
{
  "multi_level_headers": [
    ["Quarter", "Sales", "Sales", "Expenses", "Expenses"],  // Level 0
    ["Unnamed: 0_level_1", "Domestic", "International", "Fixed", "Variable"]  // Level 1
  ]
}
```

**éªŒè¯**: âœ… Pandas å°† colspan è½¬ä¸ºå¤šå±‚ MultiIndex columnsï¼

---

**3. æ··åˆåœºæ™¯ï¼ˆTest 4ï¼‰**

åŸå§‹ HTMLï¼š6 ä¸ªåˆå¹¶å•å…ƒæ ¼ï¼ˆ2 ä¸ª rowspan + 4 ä¸ª colspanï¼‰

Pandas è¾“å‡ºï¼š
```json
{
  "shape": {"rows": 4, "columns": 8},  // â† å®Œç¾çš„çŸ©å½¢
  "multi_level_headers": [
    ["Region", "Product", "2023", "2023", "2023", "2024", "2024", "2024"],
    ["Region", "Product", "Q1", "Q2", "Q3", "Q1", "Q2", "Q3"]
  ],
  "rows": [
    ["North", "Product A", "100", "110", "120", "130", "140", "150"],
    ["North", "Product B", "80", "85", "90", "95", "100", "105"],  // â† "North" å¤åˆ¶
    ...
  ]
}
```

**éªŒè¯**: âœ… åŒæ—¶å¤„ç† rowspan å’Œ colspanï¼

---

**4. 3 å±‚è¡¨å¤´ï¼ˆTest 5ï¼‰**

Pandas è¾“å‡ºï¼š
```json
{
  "multi_level_headers": [
    ["Year", "Financial Metrics", "Financial Metrics", ..., "Notes"],  // Level 0
    ["Year", "Revenue", "Revenue", "Revenue", "Profit", ..., "Notes"], // Level 1
    ["Year", "Actual", "Budget", "Variance", "Actual", ..., "Notes"]   // Level 2
  ]
}
```

**éªŒè¯**: âœ… å®Œç¾è§£æ 3 å±‚åµŒå¥—è¡¨å¤´ï¼

---

#### **æœ€ç»ˆç»“è®º**

**Pandas æ˜¯æœ€ä½³é€‰æ‹©**ï¼ŒåŸå› ï¼š

1. âœ… **é›¶æ‰‹å·¥å¤„ç†**: rowspan/colspan è‡ªåŠ¨å±•å¼€
2. âœ… **é²æ£’æ€§å¼º**: å¤„ç†è¿‡æµ·é‡çœŸå®åœºæ™¯ï¼ˆé‡‘èã€ç§‘ç ”æ•°æ®ï¼‰
3. âœ… **æ ‡å‡†åŒ–è¾“å‡º**: DataFrame å¯è½»æ¾è½¬ä¸ºä»»ä½•æ ¼å¼
4. âœ… **ç”Ÿæ€æˆç†Ÿ**: Python æ•°æ®åˆ†æäº‹å®æ ‡å‡†
5. âœ… **ä»£ç ç®€æ´**: 3 è¡Œä»£ç è§£å†³ BeautifulSoup éœ€è¦ 50+ è¡Œçš„é—®é¢˜

**å¯¹æ¯” BeautifulSoup**:

```python
# Pandasï¼ˆ3 è¡Œï¼‰
import pandas as pd
df = pd.read_html(html)[0]
data = {"headers": df.columns.tolist(), "rows": df.values.tolist()}

# BeautifulSoupï¼ˆ50+ è¡Œï¼Œä¸”éœ€è¦æ‰‹åŠ¨å¤„ç† rowspan/colspanï¼‰
from bs4 import BeautifulSoup
soup = BeautifulSoup(html, 'html.parser')
# ... å¤æ‚çš„ rowspan/colspan å¤„ç†é€»è¾‘ï¼ˆéœ€è¦è·Ÿè¸ªå•å…ƒæ ¼ä½ç½®ï¼‰
# ... éœ€è¦å¤„ç†å¤šå±‚è¡¨å¤´
# ... éœ€è¦å¤„ç†ç©ºå•å…ƒæ ¼
```

**å†³ç­–**: âœ… **åœ¨ TableExtractor ä¸­ä½¿ç”¨ Pandas ä½œä¸ºä¸»è§£æå™¨**

---

## ç›¸å…³æ–‡ä»¶

**å½“å‰æ¶‰åŠçš„æ–‡ä»¶**:
- `zag/readers/mineru.py` - MinerU Reader
- `zag/readers/docling.py` - Docling Reader
- `zag/splitters/markdown/header_based.py` - Markdown æ ‡é¢˜åˆ‡åˆ†å™¨
- `zag/extractors/table.py` - è¡¨æ ¼æå–å™¨
- `zag/schemas/unit.py` - Unit å®šä¹‰
- `zag/schemas/pdf.py` - PDF å’Œ Page å®šä¹‰

**éœ€è¦æ–°å»ºçš„æ–‡ä»¶**ï¼ˆå¯é€‰ï¼‰:
- `zag/splitters/markdown/table_aware.py` - è¡¨æ ¼æ„ŸçŸ¥åˆ‡åˆ†å™¨
- `zag/extractors/vlm_table.py` - VLM è¡¨æ ¼æå–å™¨

---

## æ€»ç»“

è¿™ä¸ªæŒ‘æˆ˜çš„æ ¸å¿ƒåœ¨äºï¼š**ä¸åŒç»„ä»¶ä¹‹é—´çš„èŒè´£åˆ’åˆ†å’Œæ•°æ®ä¼ é€’**ã€‚

- **Reader**: è´Ÿè´£é«˜ç²¾åº¦è§£æï¼Œè¯†åˆ«è¡¨æ ¼å¹¶æ„å»º TableUnit
- **Splitter**: è´Ÿè´£æŒ‰è¯­ä¹‰åˆ‡åˆ†æ–‡æœ¬ï¼Œä½†ä¸éœ€è¦é‡å¤è§£æè¡¨æ ¼
- **Extractor**: è´Ÿè´£ç†è§£è¡¨æ ¼è¯­ä¹‰ï¼Œç”Ÿæˆé«˜è´¨é‡æ‘˜è¦

é€šè¿‡åœ¨ **Reader å±‚é¢æ„å»º TableUnit**ï¼Œå¯ä»¥æœ€å¤§ç¨‹åº¦é¿å…é‡å¤è§£æï¼ŒåŒæ—¶ä¸ºåç»­çš„ Extractorã€Embedderã€Retriever æä¾›ä¸°å¯Œçš„ç»“æ„åŒ–ä¿¡æ¯ã€‚
